{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8740,
     "status": "ok",
     "timestamp": 1666177471791,
     "user": {
      "displayName": "Purbid",
      "userId": "10935929318913946630"
     },
     "user_tz": -330
    },
    "id": "cmSXh4eWDtP9",
    "outputId": "a9934572-1009-4be6-c40e-1caf1d617018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.3 MB 14.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 58.2 MB/s \n",
      "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 78.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.1 transformers-4.23.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A8oTyD0s46Ag"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "import itertools\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maqMyj8R6atz"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Shift Module:\n",
    "    A Bi-LSTM is used to generate feature vectors for each sentence from the sentence embeddings. \n",
    "    The feature vectors are actually context-aware sentence embeddings. \n",
    "    These are then fed to a feed-forward network to obtain emission scores for each class at each sentence.\n",
    "'''\n",
    "class LSTM_Emitter_Binary(nn.Module):\n",
    "    def __init__(self, n_tags, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.fc1 = nn.Linear(512, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 200)\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, n_tags)\n",
    "        self.hidden = None\n",
    "        self.device = device\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n",
    "    \n",
    "    def forward(self, sequences, sequences_rhet):\n",
    "        ## sequences: tensor[batch_size, max_seq_len, emb_dim]\n",
    "        \n",
    "        # initialize hidden state\n",
    "        # print(\"sequences binary shape : {}\".format(sequences.shape))\n",
    "        # print(\"sequences rhet shape : {}\".format(sequences_rhet.shape))\n",
    "        sequences = self.fc1(sequences).to(self.device)\n",
    "        sequences = self.fc2(sequences).to(self.device)\n",
    "        # print(\"new sequences rhet shape : {}\".format(sequences.shape))\n",
    "\n",
    "\n",
    "        sequences = torch.cat((sequences, sequences_rhet), 2).to(self.device)\n",
    "\n",
    "        self.hidden = self.init_hidden(sequences.shape[0])\n",
    "        \n",
    "        # generate context-aware sentence embeddings (feature vectors)\n",
    "        ## tensor[batch_size, max_seq_len, emb_dim] --> tensor[batch_size, max_seq_len, hidden_dim]\n",
    "        x, self.hidden = self.lstm(sequences, self.hidden)\n",
    "        x_new = self.dropout(x)\n",
    "        \n",
    "        # generate emission scores for each class at each sentence\n",
    "        # tensor[batch_size, max_seq_len, hidden_dim] --> tensor[batch_size, max_seq_len, n_tags]\n",
    "        x_new = self.hidden2tag(x_new)\n",
    "        return x_new, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OKYrSqC6hCh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "RR Module:\n",
    "    A Bi-LSTM is used to generate feature vectors for each sentence from the sentence embeddings. \n",
    "    The feature vectors are actually context-aware sentence embeddings. \n",
    "    These are then fed to a feed-forward network to obtain emission scores for each class at each sentence.\n",
    "'''\n",
    "class LSTM_Emitter(nn.Module):\n",
    "    def __init__(self, n_tags, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.hidden2tag = nn.Linear(2*hidden_dim, n_tags)\n",
    "        # self.hidden2tag = nn.Linear(hidden_dim, n_tags)\n",
    "        self.hidden = None\n",
    "        self.device = device\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n",
    "    \n",
    "    def forward(self, sequences, hidden_binary):\n",
    "    \n",
    "        ## sequences: tensor[batch_size, max_seq_len, emb_dim]\n",
    "        \n",
    "        # initialize hidden state\n",
    "        self.hidden = self.init_hidden(sequences.shape[0])\n",
    "        \n",
    "        # generate context-aware sentence embeddings (feature vectors)\n",
    "        ## tensor[batch_size, max_seq_len, emb_dim] --> tensor[batch_size, max_seq_len, hidden_dim]\n",
    "        x, self.hidden = self.lstm(sequences, self.hidden)\n",
    "        final = torch.zeros((x.shape[0], x.shape[1], 2*x.shape[2])).to(self.device)\n",
    "        ## Concat the hidden states of both Shift and RR Module LSTM's and then pass through a linear layer to get emission scores for RR Module\n",
    "        for batch_name, doc in enumerate(x):\n",
    "            for i, sent in enumerate(doc):\n",
    "                final[batch_name][i] = torch.cat((x[batch_name][i], hidden_binary[batch_name][i]),0)\n",
    "        final = self.dropout(final)\n",
    "\n",
    "        # generate emission scores for each class at each sentence\n",
    "        # tensor[batch_size, max_seq_len, hidden_dim] --> tensor[batch_size, max_seq_len, n_tags]\n",
    "        final = self.hidden2tag(final)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PRY1iI2W6lnt"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    A linear-chain CRF is fed with the emission scores at each sentence, \n",
    "    and it finds out the optimal sequence of tags by learning the transition scores.\n",
    "'''\n",
    "class CRF(nn.Module):    \n",
    "    def __init__(self, n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_tags = n_tags\n",
    "        self.SOS_TAG_IDX = sos_tag_idx\n",
    "        self.EOS_TAG_IDX = eos_tag_idx\n",
    "        self.PAD_TAG_IDX = pad_tag_idx\n",
    "        \n",
    "        self.transitions = nn.Parameter(torch.empty(self.n_tags, self.n_tags))\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        # initialize transitions from random uniform distribution between -0.1 and 0.1\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "        \n",
    "        # enforce constraints (rows = from, cols = to) with a big negative number.\n",
    "        # exp(-1000000) ~ 0\n",
    "        \n",
    "        # no transitions to SOS\n",
    "        self.transitions.data[:, self.SOS_TAG_IDX] = -1000000.0\n",
    "        # no transition from EOS\n",
    "        self.transitions.data[self.EOS_TAG_IDX, :] = -1000000.0\n",
    "        \n",
    "        if self.PAD_TAG_IDX is not None:\n",
    "            # no transitions from pad except to pad\n",
    "            self.transitions.data[self.PAD_TAG_IDX, :] = -1000000.0\n",
    "            self.transitions.data[:, self.PAD_TAG_IDX] = -1000000.0\n",
    "            # transitions allowed from end and pad to pad\n",
    "            self.transitions.data[self.PAD_TAG_IDX, self.EOS_TAG_IDX] = 0.0\n",
    "            self.transitions.data[self.PAD_TAG_IDX, self.PAD_TAG_IDX] = 0.0\n",
    "            \n",
    "    def forward(self, emissions, tags, mask = None):\n",
    "        ## emissions: tensor[batch_size, seq_len, n_tags]\n",
    "        ## tags: tensor[batch_size, seq_len]\n",
    "        ## mask: tensor[batch_size, seq_len], indicates valid positions (0 for pad)\n",
    "        return -self.log_likelihood(emissions, tags, mask = mask)\n",
    "    \n",
    "    def log_likelihood(self, emissions, tags, mask = None):                   \n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2])\n",
    "            \n",
    "        scores = self._compute_scores(emissions, tags, mask = mask)\n",
    "        partition = self._compute_log_partition(emissions, mask = mask)\n",
    "        return torch.sum(scores - partition)\n",
    "    \n",
    "    # find out the optimal tag sequence using Viterbi Decoding Algorithm\n",
    "    def decode(self, emissions, mask = None):      \n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2])\n",
    "            \n",
    "        scores, sequences = self._viterbi_decode(emissions, mask)\n",
    "        return scores, sequences\n",
    "    \n",
    "    def _compute_scores(self, emissions, tags, mask):\n",
    "        batch_size, seq_len = tags.shape\n",
    "        if(torch.cuda.is_available()):\n",
    "            scores = torch.zeros(batch_size).cuda()\n",
    "        else:\n",
    "            scores = torch.zeros(batch_size)\n",
    "        \n",
    "        # save first and last tags for later\n",
    "        first_tags = tags[:, 0]\n",
    "        last_valid_idx = mask.int().sum(1) - 1\n",
    "        last_tags = tags.gather(1, last_valid_idx.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        # add transition from SOS to first tags for each sample in batch\n",
    "        t_scores = self.transitions[self.SOS_TAG_IDX, first_tags]\n",
    "        \n",
    "        # add emission scores of the first tag for each sample in batch\n",
    "        e_scores = emissions[:, 0].gather(1, first_tags.unsqueeze(1)).squeeze()\n",
    "        scores += e_scores + t_scores\n",
    "        \n",
    "        # repeat for every remaining word\n",
    "        for i in range(1, seq_len):\n",
    "            \n",
    "            is_valid = mask[:, i]\n",
    "            prev_tags = tags[:, i - 1]\n",
    "            curr_tags = tags[:, i]\n",
    "            \n",
    "            e_scores = emissions[:, i].gather(1, curr_tags.unsqueeze(1)).squeeze()\n",
    "            t_scores = self.transitions[prev_tags, curr_tags]\n",
    "                        \n",
    "            # apply the mask\n",
    "            e_scores = e_scores * is_valid\n",
    "            t_scores = t_scores * is_valid\n",
    "            \n",
    "            scores += e_scores + t_scores\n",
    "            \n",
    "        # add transition from last tag to EOS for each sample in batch\n",
    "        scores += self.transitions[last_tags, self.EOS_TAG_IDX]\n",
    "        return scores\n",
    "    \n",
    "    # compute the partition function in log-space using forward algorithm\n",
    "    def _compute_log_partition(self, emissions, mask):\n",
    "        batch_size, seq_len, n_tags = emissions.shape\n",
    "        \n",
    "        # in the first step, SOS has all the scores\n",
    "        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n",
    "            e_scores = emissions[:, i].unsqueeze(1) \n",
    "            \n",
    "            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n",
    "            t_scores = self.transitions.unsqueeze(0)\n",
    "            \n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n",
    "            a_scores = alphas.unsqueeze(2)\n",
    "            \n",
    "            scores = e_scores + t_scores + a_scores\n",
    "            new_alphas = torch.logsumexp(scores, dim = 1)\n",
    "            \n",
    "            # set alphas if the mask is valid, else keep current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n",
    "            \n",
    "        # add scores for final transition\n",
    "        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n",
    "        end_scores = alphas + last_transition.unsqueeze(0)\n",
    "        \n",
    "        # return log_sum_exp\n",
    "        return torch.logsumexp(end_scores, dim = 1)\n",
    "    \n",
    "    # return a list of optimal tag sequence for each example in the batch\n",
    "    def _viterbi_decode(self, emissions, mask):\n",
    "        batch_size, seq_len, n_tags = emissions.shape\n",
    "        \n",
    "        # in the first iteration, SOS will have all the scores and then, the max\n",
    "        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n",
    "        \n",
    "        backpointers = []\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n",
    "            e_scores = emissions[:, i].unsqueeze(1) \n",
    "            \n",
    "            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n",
    "            t_scores = self.transitions.unsqueeze(0)\n",
    "            \n",
    "            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n",
    "            a_scores = alphas.unsqueeze(2)\n",
    "            \n",
    "            scores = e_scores + t_scores + a_scores\n",
    "            \n",
    "            # find the highest score and tag, instead of log_sum_exp\n",
    "            max_scores, max_score_tags = torch.max(scores, dim = 1)\n",
    "            \n",
    "            # set alphas if the mask is valid, otherwise keep the current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * max_scores + (1 - is_valid) * alphas\n",
    "            \n",
    "            backpointers.append(max_score_tags.t())\n",
    "            \n",
    "        # add scores for final transition\n",
    "        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n",
    "        end_scores = alphas + last_transition.unsqueeze(0)\n",
    "\n",
    "        # get the final most probable score and the final most probable tag\n",
    "        max_final_scores, max_final_tags = torch.max(end_scores, dim=1)\n",
    "\n",
    "        # find the best sequence of labels for each sample in the batch\n",
    "        best_sequences = []\n",
    "        emission_lengths = mask.int().sum(dim=1)\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # recover the original sentence length for the i-th sample in the batch\n",
    "            sample_length = emission_lengths[i].item()\n",
    "\n",
    "            # recover the max tag for the last timestep\n",
    "            sample_final_tag = max_final_tags[i].item()\n",
    "\n",
    "            # limit the backpointers until the last but one\n",
    "            # since the last corresponds to the sample_final_tag\n",
    "            sample_backpointers = backpointers[: sample_length - 1]\n",
    "\n",
    "            # follow the backpointers to build the sequence of labels\n",
    "            sample_path = self._find_best_path(i, sample_final_tag, sample_backpointers)\n",
    "\n",
    "            # add this path to the list of best sequences\n",
    "            best_sequences.append(sample_path)\n",
    "\n",
    "        return max_final_scores, best_sequences\n",
    "    \n",
    "    # auxiliary function to find the best path sequence for a specific example\n",
    "    def _find_best_path(self, sample_id, best_tag, backpointers):\n",
    "        ## backpointers: list[tensor[seq_len_i - 1, n_tags, batch_size]], seq_len_i is the length of the i-th sample of the batch\n",
    "        \n",
    "        # add the final best_tag to our best path\n",
    "        best_path = [best_tag]\n",
    "\n",
    "        # traverse the backpointers in backwards\n",
    "        for backpointers_t in reversed(backpointers):\n",
    "\n",
    "            # recover the best_tag at this timestep\n",
    "            best_tag = backpointers_t[best_tag][sample_id].item()\n",
    "\n",
    "            # append to the beginning of the list so we don't need to reverse it later\n",
    "            best_path.insert(0, best_tag)\n",
    "\n",
    "        return best_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6p9uq-DAJPPc"
   },
   "source": [
    "ATTENTION HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LMgmEOveJRVy"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "def attention_padding_mask(q, k, padding_index=0):\n",
    "    \"\"\"Generate mask tensor for padding value\n",
    "    Args:\n",
    "        q (Tensor): (B, T_q)\n",
    "        k (Tensor): (B, T_k)\n",
    "        padding_index (int): padding index. Default: 0\n",
    "    Returns:\n",
    "        (torch.BoolTensor): Mask with shape (B, T_q, T_k). True element stands for requiring making.\n",
    "    Notes:\n",
    "        Assume padding_index is 0:\n",
    "        k.eq(0) -> BoolTensor (B, T_k)\n",
    "        k.eq(0).unsqueeze(1)  -> (B, 1, T_k)\n",
    "        k.eq(0).unsqueeze(1).expand(-1, q.size(-1), -1) -> (B, T_q, T_k)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    q = torch.mean(q,2)\n",
    "\n",
    "    mask = k.eq(padding_index).unsqueeze(1).expand(-1, q.size(-1), -1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot-product attention calculation\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads = 2, dropout_rate=0.0, **kwargs):\n",
    "        \"\"\"Initialize ScaledDotProductAttention\n",
    "        Args:\n",
    "            dropout_rate (float): attention dropout_rate rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        print(\"inside scaled dot prod attention\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        # self.merged_heads = nn.Linear(num_heads, 1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask = None):\n",
    "\n",
    "      scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(K.size(-1)) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
    "      scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
    "      attn = nn.Softmax(dim=-1)(scores)\n",
    "      attn = self.dropout(attn)\n",
    "      context = torch.matmul(attn, V)\n",
    "      \n",
    "      # attention_weights = self.merged_heads(attn.permute(0,2,3,1))\n",
    "      return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim=512, num_heads=4, dropout_rate=0.0, attention_type='scaled_dot', query_key_value_weights = [], device = \"cuda\"):\n",
    "        super().__init__()\n",
    "        assert model_dim % num_heads == 0, 'model_dim should be devided by num_heads'\n",
    "        self.h_size = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.device = device\n",
    "        self.head_h_size = model_dim // num_heads\n",
    "\n",
    "        self.linear_q = nn.Linear(self.h_size, self.h_size)\n",
    "        self.linear_k = nn.Linear(self.h_size, self.h_size)\n",
    "        self.linear_v = nn.Linear(self.h_size, self.h_size)\n",
    "        self.fc0 = nn.Linear(self.h_size, self.h_size)\n",
    "\n",
    "        ## positional encoding to be added\n",
    "        # self.positional_encoder = PositionalEncoding(self.h_size).to(self.device)\n",
    "        ### newly added dropout\n",
    "        self.attention = ScaledDotProductAttention(dropout_rate = 0.2).to(self.device)\n",
    "        # self.attention = CosineAttention(dropout_rate = 0.2)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.lnorm = nn.LayerNorm(model_dim)\n",
    "\n",
    "      \n",
    "\n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        batch_size = q.size(0)\n",
    "\n",
    "        tensor1 = []\n",
    "      \n",
    "\n",
    "        # Residual\n",
    "        residual = q\n",
    "\n",
    "        # q, k, v = self.add_positional_mask(q, k, v)\n",
    "\n",
    "        # Linear projection\n",
    "        q = self.linear_q(q)\n",
    "        k = self.linear_k(k)\n",
    "        v = self.linear_v(v)\n",
    "\n",
    "        # Form multi heads\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.head_h_size).transpose(1,2)  # (h * B, T_q, D / h)\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.head_h_size).transpose(1,2)  # (h * B, T_k, D / h)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.head_h_size).transpose(1,2)  # (h * B, T_v, D / h)\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(1).repeat(1, self.num_heads, 1, 1).to(self.device)  # (h * B, T_q, T_k)\n",
    "\n",
    "       \n",
    "        \n",
    "        context, attention_per_head = self.attention(q, k, v, attn_mask=attn_mask)\n",
    "        # context: (h * B, T_q, D_v) attention: (h * B, T_q, T_k)\n",
    "\n",
    "        # Concatenate heads\n",
    "        context = context.view(batch_size, -1, self.h_size)  # (B, T_q, D)\n",
    "\n",
    "        \n",
    "        # Dropout\n",
    "        output = self.dropout(self.fc0(context))  # (B, T_q, D)\n",
    "\n",
    "        # Residual connection and Layer Normalization\n",
    "        output = self.lnorm(residual + output)  # (B, T_q, D)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5nmrTmNGwVeA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwpBV6VC6zDc"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    MTL Model to classify. Our Architecture which used the RR component and \n",
    "    Shift component parallely to get the emission scores and then they are \n",
    "    fed into the CRF to get the appropriate probabilities for each label.\n",
    "'''\n",
    "class MTL_Classifier(nn.Module):\n",
    "    def __init__(self, n_tags, sent_emb_dim, sos_tag_idx, eos_tag_idx, pad_tag_idx, vocab_size = 0, pad_word_idx = 0, pretrained = False, device = 'cuda', use_attention = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = sent_emb_dim\n",
    "        self.pretrained = pretrained\n",
    "        self.use_attention = use_attention\n",
    "        self.device = device\n",
    "        self.pad_tag_idx = pad_tag_idx\n",
    "        self.pad_word_idx = pad_word_idx\n",
    "\n",
    "\n",
    "        ##### attention code ##########\n",
    "        if self.use_attention:\n",
    "\n",
    "          attention_type = \"scaled_dot\"\n",
    "          self.attention_heads = 5\n",
    "          self.num_blocks = 2\n",
    "          self.dropout_rate = 0.2\n",
    "\n",
    "\n",
    "          multi_headed_attention_weights = []\n",
    "\n",
    "          for i in range(self.num_blocks):\n",
    "              self.__setattr__('multihead_attn_{}'.format(i), MultiHeadAttention(model_dim=self.emb_dim,\n",
    "                                                                                num_heads=self.attention_heads,\n",
    "                                                                                dropout_rate=self.dropout_rate,\n",
    "                                                                                attention_type=attention_type,\n",
    "                                                                                query_key_value_weights = multi_headed_attention_weights,\n",
    "                                                                                device=self.device))\n",
    "\n",
    "        ##### attention code ##########\n",
    "\n",
    "\n",
    "        if not self.pretrained:\n",
    "\n",
    "\n",
    "          self.rhetorical_encoder = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\n",
    "          count = 0\n",
    "          for name, param in (self.rhetorical_encoder).named_parameters():\n",
    "              # if\n",
    "              count = count + 1\n",
    "              if count <= 84:\n",
    "                  param.requires_grad = False\n",
    "              else:\n",
    "                  param.requires_grad = True\n",
    "\n",
    "\n",
    "\n",
    "        ## RR Model\n",
    "        self.emitter = LSTM_Emitter(n_tags, sent_emb_dim, sent_emb_dim, 0.5, self.device).to(self.device)\n",
    "        self.crf = CRF(n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n",
    "        \n",
    "        ## Shift or Binary Module\n",
    "        self.emitter_binary = LSTM_Emitter_Binary(5, 2*sent_emb_dim, sent_emb_dim, 0.5, self.device).to(self.device)\n",
    "        self.crf_binary = CRF(5, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, x, x_binary, y=[]):\n",
    "        batch_size = len(x)\n",
    "        seq_lengths = [len(doc) for doc in x]\n",
    "        max_seq_len = max(seq_lengths)\n",
    "\n",
    "\n",
    "\n",
    "        if not self.pretrained:  ## x: list[batch_size, sents_per_doc, words_per_sent]\n",
    "            # tensor_x = self.encoder(x)\n",
    "            tensor_x = []\n",
    "            for doc in x:\n",
    "                \n",
    "        \n",
    "                sents = [torch.tensor(s, dtype=torch.long) for s in doc]\n",
    "                sent_lengths = [len(s) for s in doc]\n",
    "\n",
    "                ## list[sents_per_doc, words_per_sent] --> tensor[sents_per_doc, max_sent_len]\n",
    "                sents = nn.utils.rnn.pad_sequence(sents, batch_first=True, padding_value=self.pad_word_idx).to(\n",
    "                    self.device)\n",
    "                \n",
    "                hidden_reps = self.rhetorical_encoder(sents)\n",
    "                hidden = hidden_reps[0][:, 0, :self.emb_dim]\n",
    "                tensor_x.append(hidden)\n",
    "        else:  ## x: list[batch_size, sents_per_doc, sent_emb_dim]\n",
    "            tensor_x = [torch.tensor(doc, dtype=torch.float, requires_grad=True) for doc in x]\n",
    "        \n",
    "        \n",
    "        tensor_x_binary = [torch.tensor(doc, dtype = torch.float, requires_grad = True) for doc in x_binary]\n",
    "        \n",
    "        tensor_x = nn.utils.rnn.pad_sequence(tensor_x, batch_first = True).to(self.device)    \n",
    "\n",
    "        tensor_x_binary = nn.utils.rnn.pad_sequence(tensor_x_binary, batch_first = True).to(self.device)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.emissions_binary, self.hidden_binary = self.emitter_binary(tensor_x_binary, tensor_x)\n",
    "\n",
    "\n",
    "        self.mask = torch.zeros(batch_size, max_seq_len).to(self.device)\n",
    "        for i, sl in enumerate(seq_lengths):\n",
    "            self.mask[i, :sl] = 1\n",
    "        \n",
    "        \n",
    "        if self.use_attention:\n",
    "          ######### added attention on the ouptput ##################\n",
    "\n",
    "      \n",
    "          # if y !=[]:\n",
    "          y_list = list(zip(*itertools.zip_longest(*y, fillvalue=0)))\n",
    "          y_tensor = torch.as_tensor(y_list)\n",
    "          attn_mask = attention_padding_mask(tensor_x, y_tensor, padding_index=0)  # (B, T, T)\n",
    "          attn_mask = attn_mask.to(self.device)\n",
    "          \n",
    "          for i in range(self.num_blocks):\n",
    "              tensor_x = self.__getattr__('multihead_attn_{}'.format(i))(tensor_x, tensor_x, tensor_x,\n",
    "                                                                            attn_mask=attn_mask)  # (B, T, D)\n",
    "\n",
    "\n",
    "          self.mask = torch.zeros(batch_size, max_seq_len).to(self.device)\n",
    "          for i, sl in enumerate(seq_lengths):\n",
    "              self.mask[i, :sl] = 1\n",
    "\n",
    "\n",
    "          ########## added attention on the output ################\n",
    "\n",
    "    \n",
    "        \n",
    "        ## Get hidden states of Shift Module and pass them to the RR Module for emission score calculation for RR Module\n",
    "        \n",
    "    \n",
    "        self.emissions = self.emitter(tensor_x, self.hidden_binary)\n",
    "        # self.emissions = self.emitter(tensor_x, [])\n",
    "        \n",
    "        ## Passing the emission scores to the CRF to get the final sequence of tags\n",
    "        _, path = self.crf.decode(self.emissions, mask = self.mask)\n",
    "        _, path_binary = self.crf_binary.decode(self.emissions_binary, mask = self.mask)\n",
    "        return path, path_binary\n",
    "    \n",
    "    def _loss(self, y):\n",
    "        ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n",
    "        tensor_y = [torch.tensor(doc, dtype = torch.long) for doc in y]\n",
    "        tensor_y = nn.utils.rnn.pad_sequence(tensor_y, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n",
    "        \n",
    "        nll = self.crf(self.emissions, tensor_y, mask = self.mask)\n",
    "        return nll    \n",
    "    \n",
    "    def _loss_binary(self, y_binary):\n",
    "        ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n",
    "        tensor_y_binary = [torch.tensor(doc, dtype = torch.long) for doc in y_binary]\n",
    "        tensor_y_binary = nn.utils.rnn.pad_sequence(tensor_y_binary, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n",
    "        \n",
    "        nll_binary = self.crf_binary(self.emissions_binary, tensor_y_binary, mask = self.mask)\n",
    "        return nll_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219,
     "referenced_widgets": [
      "a7bdf693bb0c4ed29c631c4ce168411a",
      "30425f1dce484a6e80572a7965999103",
      "b910d4d50de14566935a0937de30ab66",
      "6e2d9d9f5c9346bdb379771451ce8579",
      "de43a10bc6724dd9ada9cf17b2be0d98",
      "f6acdda010b0486ea9497b2795294c3b",
      "658cce8da84448bea88b281eb8d4a857",
      "2bde3aee2aeb4de482b5621716fc1163",
      "d54b6db4c14f47f4b4caeb55e22cf555",
      "87adea8c5a4f47999722abf1959125ed",
      "a96b18980f164452a3b6c24e560ed62a",
      "ce08c87dc91a42c98ca1bd9d219153d8",
      "039b1fad52fb408a90bdc709eabe7642",
      "3499bf73f6db458fb62d81abff324420",
      "336e2b56c9fe48c6bc53d1f059a8da67",
      "75cd49754b924524b54327ffaae744c3",
      "277b11c516544b8c9c9cf311acc6aec7",
      "91f0b2f5c53143a39bdbc5e6e401a6fd",
      "91142443e2984c5ba8a302c5a0a56a0b",
      "4e5a47991c7640e1873134784203c11d",
      "aabfc5afb31d40709ed4dcf7128a1965",
      "788d5c013441489aad124ff9e60909b8",
      "1ed456ab88e74ad3bc4ce9c9901771ff",
      "c8286db66aeb41e6904d0361d739bfb4",
      "f14b6d1bf1de457b910b75d3833867b0",
      "863921a12ead4cd5a4c9612f9145d0a2",
      "9a2b8b70eaa54ee69d47b062af5cd5a2",
      "430a3c25877640d98a7ea30b724f6cfc",
      "f50ba7b51e1c4a1a93ece757c460a956",
      "1c184ea55e2c4c91b1e339fd03ded3af",
      "942357d76bc348cdb1997435ca34bb38",
      "43730558f55447509dbd6bd47bfaaeac",
      "0e23dbafd79645208a4e3f3d93e5adf0",
      "1a5a514166fa4e7a95ba5056575f6650",
      "a478037f98cb4ff896e916398c34c2c3",
      "af9c483bed814ef2afe2ce2fa22ae2ad",
      "cfb19276ebed4e89bec10bf8baf48d3a",
      "28cb0db6f1dc4c1cada2d753113b42e5",
      "9e10223f73ad4eac821e64ddfb358c67",
      "dd82a38d534d41a3825caed3fb602228",
      "7427b98095244a54b223e102bef31b1a",
      "c384cd731a7d456e945b0e578b4c073f",
      "771c5c55fb144e959826234b3c8275f0",
      "f79e154945ae488493cb978128576567"
     ]
    },
    "executionInfo": {
     "elapsed": 8265,
     "status": "ok",
     "timestamp": 1666177500368,
     "user": {
      "displayName": "Purbid",
      "userId": "10935929318913946630"
     },
     "user_tz": -330
    },
    "id": "COGcRAUW65fT",
    "outputId": "d93861c9-6af9-423d-81a0-21bb49236c1e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bdf693bb0c4ed29c631c4ce168411a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce08c87dc91a42c98ca1bd9d219153d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/989 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed456ab88e74ad3bc4ce9c9901771ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a5a514166fa4e7a95ba5056575f6650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/141M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at nlpaueb/legal-bert-small-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-small-uncased')\n",
    "encoder = AutoModel.from_pretrained('nlpaueb/legal-bert-small-uncased')\n",
    "\n",
    "\n",
    "def prepare_folds(args):\n",
    "    with open(args.cat_path) as fp:\n",
    "        \n",
    "        categories = []\n",
    "        for line in fp:\n",
    "            _, docs = line.strip().split('\\t')\n",
    "            docs = docs.strip().split(' ')\n",
    "            categories.append(docs)\n",
    "\n",
    "    # categories: list[category, docs_per_category]\n",
    "\n",
    "    categories.sort(key = lambda x: len(x))\n",
    "    n_docs = len(sum(categories, []))\n",
    "    print(n_docs)\n",
    "    assert n_docs == args.dataset_size, \"invalid category list\"\n",
    "           \n",
    "    docs_per_fold = args.dataset_size // args.num_folds   \n",
    "    folds = [[] for f in range(docs_per_fold)]\n",
    "    print(folds)\n",
    "    \n",
    "    # folds: list[num_folds, docs_per_fold]\n",
    "    \n",
    "    f = 0\n",
    "    for cat in categories:\n",
    "        for doc in cat:\n",
    "            folds[f].append(doc)\n",
    "            f = (f + 1) % 5\n",
    "\n",
    "    # list[num_folds, docs_per_fold] --> list[num_folds * docs_per_fold]\n",
    "    idx_order = sum(folds, [])\n",
    "    return idx_order\n",
    "\n",
    "\n",
    "'''\n",
    "    This function prepares the numericalized data in the form of lists, to be used for training, test and evaluation.\n",
    "        x:  list[num_docs, sentences_per_doc, sentence_embedding_dim] \n",
    "        y:  list[num_docs, sentences_per_doc]\n",
    "'''\n",
    "def prepare_data_new(idx_order, args, data_path, tag2idx=None, data_type = 'rhetoric'):\n",
    "    x, y = [], []\n",
    "    \n",
    "    \n",
    "    \n",
    "    word2idx = defaultdict(lambda: len(word2idx))\n",
    "    if tag2idx is None:\n",
    "        tag2idx = defaultdict(lambda: len(tag2idx))\n",
    "        tag2idx['<pad>'], tag2idx['<start>'], tag2idx['<end>'] = 0, 1, 2\n",
    "    \n",
    "\n",
    "    # map the special symbols first\n",
    "    if data_type == 'binary':\n",
    "      word2idx['<pad>'], word2idx['<unk>'] = 0, 1\n",
    "    else:\n",
    "      word2idx['<pad>'], word2idx['<unk>'], word2idx['[CLS]'], word2idx['[SEP]'] = 0, 1, 2, 3\n",
    "\n",
    "\n",
    "    # iterate over documents\n",
    "    for doc in idx_order:\n",
    "        doc_x, doc_y = [], [] \n",
    "\n",
    "        \n",
    "        if data_type == 'binary':\n",
    "          \n",
    "          with open(data_path + doc + '.txt') as fp:\n",
    "            full_curr_doc = fp.readlines()\n",
    "            for sent_num, sent in enumerate(full_curr_doc):\n",
    "                    # if sent_num == 0:\n",
    "                    #   continue\n",
    "                    sent_x, sent_y = sent.strip().split('\\t')\n",
    "                    if 'tensor(0)' in sent_y:\n",
    "                      sent_y = '0'\n",
    "                    elif 'tensor(1)' in sent_y:\n",
    "                      sent_y = '1'\n",
    "\n",
    "                    sent_x_curr_and_next = list(map(float, sent_x.strip().split()[:args.shift_emb_dim]))\n",
    "                    # sent_x_curr_and_prev = list(map(float, full_curr_doc[sent_num - 1].strip().split()[:args.emb_dim]))\n",
    "                    sent_x = sent_x_curr_and_next \n",
    "                    # + sent_x_curr_and_prev\n",
    "                    sent_y = tag2idx[str(sent_y).strip()]\n",
    "\n",
    "                    if sent_x != []:\n",
    "                        doc_x.append(sent_x)\n",
    "                        doc_y.append(sent_y)\n",
    "          \n",
    "        else:\n",
    "          with open(data_path + doc + '.txt') as fp:\n",
    "              \n",
    "              # iterate over sentences\n",
    "              for sent in fp:\n",
    "                  try:\n",
    "                    sent_x, sent_y = sent.strip().split('\\t')\n",
    "                  except ValueError:\n",
    "                    continue\n",
    "\n",
    "                  # cleanse text, map words and tags\n",
    "                  if not args.pretrained:\n",
    "                    \n",
    "                    tokens = tokenizer.tokenize(sent_x)\n",
    "                  \n",
    "                    if len(tokens) >= 50:\n",
    "                        tokens = tokens[0:50]\n",
    "                    tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "                    sent_x = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "                      # sent_x = sent_x.strip().lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n",
    "                      # sent_x = list(map(lambda x: word2idx[x], sent_x.split()))\n",
    "                  \n",
    "                  else:\n",
    "                      sent_x = list(map(float, sent_x.strip().split()[:args.emb_dim]))\n",
    "                  sent_y = tag2idx[str(sent_y).strip()]\n",
    "\n",
    "                  if sent_x != []:\n",
    "                      doc_x.append(sent_x)\n",
    "                      doc_y.append(sent_y)\n",
    "          \n",
    "        # if data_type == 'rhetoric':\n",
    "        #   doc_x, doc_y = doc_x[:-1], doc_y[:-1]\n",
    "        \n",
    "        x.append(doc_x)\n",
    "        y.append(doc_y)\n",
    "   \n",
    "\n",
    "    return x, y,  word2idx, tag2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w70-66d27B2b"
   },
   "outputs": [],
   "source": [
    "'''To create batches for training'''\n",
    "def batchify(x, y, x_binary, y_binary, batch_size):\n",
    "    idx = list(range(len(x)))\n",
    "    random.shuffle(idx)\n",
    "    \n",
    "    # convert to numpy array for ease of indexing\n",
    "    x = np.array(x)[idx]\n",
    "    y = np.array(y)[idx]\n",
    "    \n",
    "    x_binary = np.array(x_binary)[idx]\n",
    "    y_binary = np.array(y_binary)[idx]\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(x):\n",
    "        j = min(i + batch_size, len(x))\n",
    "        \n",
    "        batch_idx = idx[i : j]\n",
    "        batch_x = x[i : j]\n",
    "        batch_y = y[i : j]\n",
    "        \n",
    "        batch_x_binary = x_binary[i : j]\n",
    "        batch_y_binary = y_binary[i : j]\n",
    "        \n",
    "        yield batch_idx, batch_x, batch_y, batch_x_binary, batch_y_binary\n",
    "        \n",
    "        i = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Me9s1-rv7EC3"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Perform a single training step by iterating over the entire training data once. Data is divided into batches.\n",
    "'''\n",
    "def train_step(model, opt, x, y, x_binary, y_binary, batch_size):\n",
    "    ## x: list[num_examples, sents_per_example, features_per_sentence]\n",
    "    ## y: list[num_examples, sents_per_example]\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_rhet_loss = 0\n",
    "    total_binary_loss = 0\n",
    "    y_pred = [] # predictions\n",
    "    y_gold = [] # gold standard\n",
    "    y_pred_binary = []\n",
    "    y_gold_binary = []\n",
    "    idx = [] # example index\n",
    "    mu = 0.95 # hyperparameter\n",
    "    for i, (batch_idx, batch_x, batch_y, batch_x_binary, batch_y_binary) in enumerate(batchify(x, y, x_binary, y_binary, batch_size)):\n",
    "        pred, pred_binary = model(batch_x, batch_x_binary, batch_y)\n",
    "        loss = model._loss(batch_y)  \n",
    "        loss_binary = model._loss_binary(batch_y_binary)\n",
    "        \n",
    "        overall = torch.add(torch.mul(loss, (1-mu)), torch.mul(loss_binary, mu))\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # loss.backward()\n",
    "        overall.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        total_loss += overall.item()\n",
    "        total_rhet_loss += loss.item()\n",
    "        total_binary_loss += loss_binary.item()\n",
    "        # total_loss += loss.item()\n",
    "     \n",
    "        y_pred.extend(pred)\n",
    "        y_gold.extend(batch_y)\n",
    "        y_pred_binary.extend(pred_binary)\n",
    "        y_gold_binary.extend(batch_y_binary)\n",
    "        idx.extend(batch_idx)\n",
    "        \n",
    "    assert len(sum(y, [])) == len(sum(y_pred, [])), \"Mismatch in predicted\"\n",
    "    \n",
    "    return total_loss / (i + 1), idx, y_gold, y_pred, y_gold_binary, y_pred_binary, total_rhet_loss/ (i + 1), total_binary_loss/ (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqs6hTbK7Gmh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Perform a single evaluation step by iterating over the entire training data once. Data is divided into batches.\n",
    "'''\n",
    "def val_step(model, x, y, x_binary, y_binary, batch_size):\n",
    "    ## x: list[num_examples, sents_per_example, features_per_sentence]\n",
    "    ## y: list[num_examples, sents_per_example]\n",
    "    ## Similarly for Binary data\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_rhet_loss = 0\n",
    "    total_binary_loss = 0\n",
    "    y_pred = [] # predictions\n",
    "    y_gold = [] # gold standard\n",
    "    y_pred_binary = []\n",
    "    y_gold_binary = []\n",
    "    idx = [] # example index\n",
    "    mu = 0.95\n",
    "    \n",
    "    for i, (batch_idx, batch_x, batch_y, batch_x_binary, batch_y_binary) in enumerate(batchify(x, y, x_binary, y_binary, batch_size)):\n",
    "        pred, pred_binary = model(batch_x, batch_x_binary, batch_y)\n",
    "        \n",
    "        loss = model._loss(batch_y)  \n",
    "        loss_binary = model._loss_binary(batch_y_binary)\n",
    "        \n",
    "        overall = torch.add(torch.mul(loss, (1-mu)), torch.mul(loss_binary, mu))\n",
    "        \n",
    "        total_loss += overall.item()\n",
    "        total_rhet_loss += loss.item()\n",
    "        total_binary_loss += loss_binary.item()\n",
    "     \n",
    "        y_pred.extend(pred)\n",
    "        y_gold.extend(batch_y)\n",
    "        y_pred_binary.extend(pred_binary)\n",
    "        y_gold_binary.extend(batch_y_binary)\n",
    "        idx.extend(batch_idx)\n",
    "        \n",
    "    assert len(sum(y, [])) == len(sum(y_pred, [])), \"Mismatch in predicted\"\n",
    "    \n",
    "    return total_loss / (i + 1), idx, y_gold, y_pred, y_gold_binary, y_pred_binary, total_rhet_loss/ (i + 1), total_binary_loss/ (i + 1)\n",
    "'''\n",
    "    Report all metrics in format using sklearn.metrics.classification_report\n",
    "'''\n",
    "def statistics(data_state, tag2idx):\n",
    "    idx, gold, pred = data_state['idx'], data_state['gold'], data_state['pred']\n",
    "    \n",
    "    rev_tag2idx = {v: k for k, v in tag2idx.items()}\n",
    "    tags = [rev_tag2idx[i] for i in range(len(tag2idx)) if rev_tag2idx[i] not in ['<start>', '<end>', '<pad>']]\n",
    "    \n",
    "    # flatten out\n",
    "    gold = sum(gold, [])\n",
    "    pred = sum(pred, [])\n",
    "    \n",
    "    \n",
    "    print(classification_report(gold, pred, target_names = tags, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1666178532866,
     "user": {
      "displayName": "Purbid",
      "userId": "10935929318913946630"
     },
     "user_tz": -330
    },
    "id": "33J2tFtpl0i6",
    "outputId": "47c473c8-19c1-4de8-e986-6b91b7627e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/IIT_law_ai/semantic_segmentation/MTL_SHIFT_MODEL/dataset/india_models/india_model_val_fold_0_correct_method_last_sentence_included_untrained_india_LEGALbert/\n"
     ]
    }
   ],
   "source": [
    "fold_num = 0\n",
    "shift_emb_dim_model = 512\n",
    "\n",
    "class Args:\n",
    "    pretrained = True\n",
    "    use_attention = True\n",
    "    data_path_rr = '/content/drive/MyDrive/IIT_law_ai/semantic_segmentation/pretrained_embeddings/IN-train-set/' ## Input to the pre=trained embedding(should contain 4 sub-folders, IT test and train, CL test and train)\n",
    "    # data_path_rr = '/content/drive/MyDrive/IIT_law_ai/semantic_segmentation/dataset/UK-train-set/'\n",
    "    data_path_binary = '/content/drive/MyDrive/IIT_law_ai/semantic_segmentation/MTL_SHIFT_MODEL/dataset/india_models/india_model_val_fold_0_correct_method_last_sentence_included_untrained_india_LEGALbert/'\n",
    "    save_path = '/content/drive/MyDrive/IIT_law_ai/semantic_segmentation/MTL_SHIFT_MODEL/saved_models/' ## path to save the model\n",
    "    cat_path = '/content/drive/MyDrive/IIT_law_ai/semantic_segmentation/in_categories.txt'\n",
    "    device = 'cuda' ## device to use\n",
    "    dataset_size = 150\n",
    "    emb_dim = 200\n",
    "    \n",
    "    ## We used a batch size of 40 for IT dataset experiments and 20 for CL and IT+CL experiments, but a larger batch size takes \n",
    "    ## too much time to train the model. So for training the model quickly we can use a smaller batch size like 6 for CL and IT+CL\n",
    "    ## and 10 for IT. The results do not vary much if we decrease the size, but to get more efficient results the batch size can \n",
    "    ## varied. Defaulting it to 10 for IT\n",
    "    num_folds = 5\n",
    "    batch_size = 4 ## batch size\n",
    "    print_every = 1 ## print loss after these many number of epochs\n",
    "    lr = 0.001 ## learning rate\n",
    "    reg = 0 ## weight decay for Adam Opt\n",
    "    shift_emb_dim = shift_emb_dim_model ## the pre-trained embedding dimension of the sentences\n",
    "    epochs = 75 ## Something between 250-300\n",
    "args = Args()\n",
    "print(args.data_path_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1666178532867,
     "user": {
      "displayName": "Purbid",
      "userId": "10935929318913946630"
     },
     "user_tz": -330
    },
    "id": "rHajQdZHmGJp",
    "outputId": "548914d0-829f-4b86-97a7-a09c6d5d2e78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "idx_order = prepare_folds(args)\n",
    "# x, y, word2idx, tag2idx = prepare_data_new(idx_order, args, data_path = args.data_path_binary, data_type = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11319923,
     "status": "ok",
     "timestamp": 1666189852787,
     "user": {
      "displayName": "Purbid",
      "userId": "10935929318913946630"
     },
     "user_tz": -330
    },
    "id": "3aqvAsMY7Oyi",
    "outputId": "5d55354e-7586-42a5-c90a-4c3f007978e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data ... Done\n",
      "#Tags Overall: 10\n",
      "#Tags Overall binary: 5\n",
      "Dump word2idx and tag2idx\n",
      "\n",
      "Initializing model for Overall ... inside scaled dot prod attention\n",
      "inside scaled dot prod attention\n",
      "Done\n",
      "\n",
      "Evaluating on test...\n",
      "running fold 0\n",
      "  EPOCH     Tr_LOSS   Tr_F1    Val_LOSS  Val_F1  Tr_bin_loss  Tr_bin_F1  Val_bin_loss  Val_bin_F1\n",
      "--------------------------------------------------------------------------------------\n",
      "      1     354.807   0.201     253.491   0.214    320.590   0.478     229.167   0.471\n",
      "      2     320.001   0.233     236.147   0.293    292.252   0.469     214.812   0.471\n",
      "      3     304.686   0.334     227.536   0.383    282.202   0.469     209.956   0.471\n",
      "      4     294.319   0.434     224.240   0.480    276.907   0.469     209.325   0.471\n",
      "      5     287.661   0.503     217.906   0.519    272.418   0.469     205.334   0.471\n",
      "      6     282.440   0.548     220.828   0.612    269.810   0.469     210.414   0.471\n",
      "      7     286.501   0.611     225.932   0.653    275.719   0.469     215.930   0.471\n",
      "      8     278.105   0.593     216.606   0.637    265.494   0.469     205.165   0.471\n",
      "      9     275.768   0.619     212.433   0.637    265.996   0.469     202.559   0.471\n",
      "     10     272.908   0.669     215.928   0.623    265.036   0.469     207.508   0.471\n",
      "     11     265.949   0.670     212.338   0.683    258.334   0.469     204.233   0.471\n",
      "     12     260.937   0.710     208.883   0.697    255.698   0.471     201.571   0.471\n",
      "     13     259.414   0.749     209.362   0.689    255.905   0.472     200.301   0.471\n",
      "     14     253.298   0.766     206.611   0.727    250.114   0.479     200.267   0.476\n",
      "     15     255.320   0.752     209.570   0.711    252.201   0.483     201.844   0.471\n",
      "     16     251.616   0.765     207.080   0.713    248.915   0.480     199.168   0.486\n",
      "     17     245.971   0.775     206.886   0.704    243.169   0.500     201.212   0.517\n",
      "     18     247.912   0.779     214.561   0.722    246.466   0.511     208.677   0.493\n",
      "     19     244.429   0.800     210.940   0.724    243.378   0.505     204.429   0.534\n",
      "     20     242.629   0.805     208.858   0.725    241.808   0.502     202.995   0.491\n",
      "     21     237.874   0.810     207.062   0.709    237.655   0.526     200.902   0.500\n",
      "     22     232.014   0.815     209.109   0.734    231.373   0.546     203.153   0.534\n",
      "     23     230.367   0.821     210.914   0.721    230.386   0.553     205.247   0.493\n",
      "     24     227.854   0.825     207.604   0.724    227.888   0.561     200.633   0.535\n",
      "     25     223.318   0.835     207.138   0.737    223.732   0.571     201.800   0.523\n",
      "     26     220.113   0.854     212.766   0.735    220.972   0.591     207.009   0.567\n",
      "     27     224.751   0.845     203.357   0.720    225.920   0.575     196.867   0.553\n",
      "     28     216.010   0.857     213.331   0.738    217.189   0.608     207.876   0.570\n",
      "     29     211.571   0.857     214.806   0.749    212.771   0.618     209.689   0.602\n",
      "     30     207.330   0.872     209.322   0.741    209.022   0.639     203.333   0.534\n",
      "     31     208.601   0.856     214.190   0.730    209.733   0.631     207.867   0.564\n",
      "     32     207.835   0.863     218.552   0.722    209.257   0.636     212.282   0.569\n",
      "     33     208.390   0.862     215.955   0.734    209.900   0.637     210.923   0.544\n",
      "     34     198.267   0.875     219.369   0.744    199.690   0.674     214.027   0.543\n",
      "     35     193.096   0.873     219.700   0.721    194.188   0.674     214.967   0.587\n",
      "     36     190.985   0.854     222.588   0.749    191.323   0.690     217.825   0.579\n",
      "     37     184.522   0.871     220.206   0.741    185.065   0.706     214.741   0.556\n",
      "     38     179.345   0.876     222.688   0.764    180.290   0.715     218.978   0.580\n",
      "     39     172.481   0.891     224.570   0.745    173.792   0.728     219.559   0.564\n",
      "     40     169.727   0.900     231.614   0.726    171.334   0.739     225.787   0.581\n",
      "     41     171.352   0.903     229.216   0.749    173.303   0.739     223.509   0.580\n",
      "     42     171.455   0.904     231.223   0.744    173.444   0.744     226.577   0.587\n",
      "     43     165.631   0.905     232.281   0.762    167.675   0.753     227.646   0.592\n",
      "     44     159.859   0.913     232.622   0.738    161.799   0.764     228.236   0.616\n",
      "     45     167.259   0.912     235.925   0.734    169.749   0.748     230.057   0.598\n",
      "     46     157.627   0.919     250.230   0.745    159.890   0.766     245.765   0.564\n",
      "     47     148.929   0.916     247.426   0.741    150.528   0.789     242.566   0.591\n",
      "     48     144.908   0.918     243.212   0.742    146.498   0.792     238.841   0.597\n",
      "     49     146.839   0.907     248.467   0.747    147.894   0.787     244.569   0.613\n",
      "     50     139.703   0.916     250.794   0.753    140.942   0.811     246.647   0.601\n",
      "     51     134.751   0.919     250.730   0.749    136.125   0.822     245.000   0.585\n",
      "     52     142.002   0.930     252.439   0.757    144.197   0.799     248.110   0.579\n",
      "     53     138.833   0.930     252.733   0.758    140.888   0.807     247.195   0.599\n",
      "     54     127.567   0.930     252.468   0.761    129.081   0.830     247.746   0.587\n",
      "     55     126.259   0.932     255.018   0.757    127.802   0.828     249.987   0.607\n",
      "     56     117.359   0.936     267.896   0.748    118.682   0.844     262.452   0.604\n",
      "     57     108.731   0.937     272.727   0.755    109.873   0.860     268.607   0.605\n",
      "     58     109.342   0.938     266.556   0.758    110.278   0.860     261.921   0.600\n",
      "     59     109.147   0.938     272.271   0.760    110.466   0.861     267.391   0.603\n",
      "     60     107.250   0.944     280.921   0.741    108.600   0.865     276.826   0.596\n",
      "     61     100.313   0.947     289.195   0.765    101.344   0.874     284.358   0.597\n",
      "     62      98.664   0.946     287.699   0.758     99.817   0.874     283.025   0.607\n",
      "     63     101.173   0.948     281.393   0.752    102.410   0.873     276.810   0.601\n",
      "     64      94.217   0.946     290.364   0.748     95.033   0.882     285.754   0.610\n",
      "     65      98.521   0.945     295.183   0.756     99.568   0.879     291.701   0.601\n",
      "     66      95.545   0.947     300.237   0.767     96.541   0.886     295.820   0.584\n",
      "     67      89.296   0.951     298.944   0.752     90.190   0.888     294.079   0.606\n",
      "     68      90.536   0.949     301.280   0.753     91.494   0.892     294.967   0.589\n",
      "     69      96.689   0.944     293.563   0.749     97.593   0.878     289.205   0.594\n",
      "     70      90.710   0.941     307.542   0.725     91.164   0.890     302.542   0.587\n",
      "     71      81.322   0.935     308.722   0.741     80.953   0.903     305.584   0.597\n",
      "     72      80.014   0.948     325.645   0.751     80.202   0.906     321.730   0.582\n",
      "     73      75.285   0.951     315.520   0.746     75.495   0.912     311.501   0.595\n",
      "     74      75.496   0.956     322.425   0.756     75.896   0.910     318.711   0.602\n",
      "     75      70.016   0.960     325.034   0.757     70.613   0.920     321.904   0.582\n",
      "Dumping model and data ... Done\n",
      "Time taken: 2327 secs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FAC      0.847     0.857     0.852      1424\n",
      "         RLC      0.837     0.641     0.726        64\n",
      "         RPC      0.937     0.881     0.908       101\n",
      "         ARG      0.805     0.640     0.713       322\n",
      "       Ratio      0.836     0.910     0.871      2830\n",
      "         PRE      0.825     0.560     0.667       521\n",
      "         STA      0.654     0.617     0.635       261\n",
      "\n",
      "    accuracy                          0.830      5523\n",
      "   macro avg      0.820     0.729     0.767      5523\n",
      "weighted avg      0.829     0.830     0.826      5523\n",
      "\n",
      "\n",
      "Initializing model for Overall ... inside scaled dot prod attention\n",
      "inside scaled dot prod attention\n",
      "Done\n",
      "\n",
      "Evaluating on test...\n",
      "running fold 1\n",
      "  EPOCH     Tr_LOSS   Tr_F1    Val_LOSS  Val_F1  Tr_bin_loss  Tr_bin_F1  Val_bin_loss  Val_bin_F1\n",
      "--------------------------------------------------------------------------------------\n",
      "      1     326.777   0.196     350.253   0.207    292.808   0.469     315.749   0.471\n",
      "      2     295.836   0.216     332.587   0.232    269.084   0.468     300.249   0.471\n",
      "      3     282.739   0.318     330.645   0.420    260.437   0.468     306.495   0.471\n",
      "      4     276.400   0.458     322.299   0.383    259.509   0.468     297.806   0.471\n",
      "      5     269.018   0.486     308.888   0.452    253.713   0.468     290.023   0.471\n",
      "      6     260.120   0.548     305.655   0.491    247.745   0.468     289.390   0.471\n",
      "      7     256.359   0.573     305.642   0.556    244.282   0.468     291.084   0.471\n",
      "      8     253.496   0.616     305.317   0.530    243.088   0.471     289.226   0.471\n",
      "      9     250.736   0.617     299.099   0.563    240.831   0.468     284.522   0.471\n",
      "     10     246.868   0.668     302.034   0.613    239.119   0.472     290.274   0.492\n",
      "     11     243.425   0.693     299.382   0.594    237.649   0.480     285.934   0.471\n",
      "     12     237.180   0.707     296.403   0.628    231.903   0.473     286.332   0.481\n",
      "     13     237.436   0.720     296.568   0.620    233.107   0.477     285.741   0.483\n",
      "     14     234.674   0.742     293.093   0.686    231.149   0.487     283.731   0.500\n",
      "     15     234.227   0.732     302.974   0.615    229.597   0.496     292.965   0.471\n",
      "     16     233.429   0.703     294.383   0.596    227.663   0.499     283.583   0.488\n",
      "     17     227.441   0.729     289.916   0.680    223.412   0.502     282.170   0.471\n",
      "     18     223.303   0.757     297.077   0.700    220.414   0.518     289.433   0.490\n",
      "     19     222.992   0.780     297.466   0.695    221.242   0.524     290.404   0.475\n",
      "     20     221.338   0.786     297.260   0.703    220.370   0.526     290.653   0.543\n",
      "     21     217.040   0.807     296.539   0.673    216.600   0.526     287.962   0.479\n",
      "     22     213.782   0.809     292.022   0.715    213.191   0.561     285.317   0.522\n",
      "     23     205.204   0.822     297.275   0.672    204.951   0.577     290.368   0.488\n",
      "     24     206.874   0.838     292.909   0.708    207.156   0.583     286.782   0.522\n",
      "     25     205.187   0.823     296.538   0.691    205.750   0.578     290.168   0.553\n",
      "     26     200.405   0.839     299.187   0.680    200.941   0.602     291.769   0.522\n",
      "     27     195.988   0.847     294.749   0.700    196.804   0.623     289.664   0.505\n",
      "     28     193.177   0.859     299.542   0.729    194.259   0.629     294.278   0.529\n",
      "     29     188.757   0.853     296.757   0.706    189.683   0.649     290.321   0.519\n",
      "     30     185.563   0.853     306.843   0.707    186.126   0.650     300.699   0.537\n",
      "     31     187.949   0.846     298.354   0.690    188.368   0.656     292.941   0.521\n",
      "     32     182.848   0.849     308.566   0.696    183.483   0.661     302.439   0.557\n",
      "     33     182.079   0.855     311.197   0.720    182.354   0.670     305.644   0.595\n",
      "     34     173.966   0.871     302.523   0.696    174.690   0.697     296.813   0.535\n",
      "     35     166.699   0.880     318.044   0.696    167.639   0.715     311.558   0.540\n",
      "     36     164.553   0.876     312.948   0.709    165.409   0.722     306.589   0.544\n",
      "     37     164.007   0.884     331.992   0.725    165.393   0.721     328.080   0.581\n",
      "     38     161.175   0.897     323.010   0.714    162.842   0.727     317.849   0.562\n",
      "     39     164.461   0.896     323.167   0.715    166.344   0.726     319.356   0.585\n",
      "     40     168.405   0.893     318.868   0.718    170.431   0.718     313.923   0.564\n",
      "     41     157.767   0.902     317.102   0.703    159.313   0.741     311.136   0.594\n",
      "     42     152.107   0.889     316.104   0.720    153.248   0.757     310.568   0.549\n",
      "     43     144.303   0.902     337.204   0.718    145.291   0.773     333.113   0.582\n",
      "     44     143.891   0.906     333.954   0.715    145.239   0.778     328.496   0.566\n",
      "     45     140.415   0.906     339.035   0.716    141.739   0.789     334.453   0.570\n",
      "     46     131.383   0.918     349.779   0.735    132.532   0.805     346.022   0.577\n",
      "     47     128.212   0.915     344.545   0.721    129.312   0.807     339.209   0.565\n",
      "     48     123.817   0.920     359.002   0.720    124.997   0.822     353.662   0.592\n",
      "     49     123.262   0.919     365.308   0.723    124.451   0.816     361.023   0.558\n",
      "     50     121.326   0.914     349.649   0.735    122.482   0.823     345.087   0.580\n",
      "     51     117.848   0.922     352.462   0.714    119.011   0.830     347.550   0.578\n",
      "     52     120.014   0.925     388.011   0.711    121.328   0.824     384.128   0.567\n",
      "     53     114.766   0.928     376.790   0.717    115.833   0.833     371.766   0.587\n",
      "     54     112.795   0.931     383.869   0.713    113.959   0.838     379.636   0.579\n",
      "     55     112.206   0.932     377.486   0.722    113.536   0.838     372.767   0.566\n",
      "     56     111.572   0.935     377.671   0.726    113.041   0.842     373.580   0.560\n",
      "     57      98.184   0.937     383.491   0.720     99.142   0.862     378.676   0.592\n",
      "     58      99.505   0.928     388.113   0.720     99.987   0.859     382.381   0.569\n",
      "     59      93.858   0.928     402.265   0.712     94.263   0.871     397.650   0.568\n",
      "     60      91.564   0.934     404.504   0.705     91.782   0.873     399.616   0.567\n",
      "     61      85.397   0.934     416.503   0.714     85.381   0.886     411.506   0.575\n",
      "     62      83.944   0.935     428.162   0.718     84.202   0.890     425.576   0.586\n",
      "     63      78.749   0.941     429.284   0.701     79.005   0.898     424.327   0.568\n",
      "     64      80.667   0.945     427.365   0.704     81.138   0.892     422.944   0.587\n",
      "     65      80.430   0.950     411.746   0.722     81.108   0.895     407.122   0.578\n",
      "     66      85.881   0.947     414.725   0.714     86.872   0.883     410.029   0.591\n",
      "     67      82.492   0.945     442.963   0.710     83.155   0.890     438.068   0.587\n",
      "     68      76.773   0.949     426.927   0.708     77.230   0.898     422.894   0.591\n",
      "     69      78.980   0.950     434.798   0.714     79.705   0.897     431.612   0.605\n",
      "     70      81.503   0.953     446.084   0.697     82.469   0.890     441.069   0.581\n",
      "     71      78.122   0.951     455.585   0.696     78.649   0.894     445.211   0.576\n",
      "     72      77.706   0.869     467.338   0.666     72.349   0.907     464.600   0.559\n",
      "     73      73.597   0.870     471.413   0.681     69.485   0.910     472.485   0.577\n",
      "     74      70.160   0.906     471.483   0.723     67.480   0.912     473.177   0.574\n",
      "     75      67.401   0.915     453.021   0.718     65.143   0.917     452.648   0.594\n",
      "Dumping model and data ... Done\n",
      "Time taken: 2071 secs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FAC      0.838     0.901     0.869      1254\n",
      "         RLC      0.914     0.763     0.831       139\n",
      "         RPC      0.877     0.732     0.798       127\n",
      "         ARG      0.667     0.409     0.507       416\n",
      "       Ratio      0.868     0.938     0.902      4665\n",
      "         PRE      0.693     0.484     0.570       742\n",
      "         STA      0.718     0.626     0.669       321\n",
      "\n",
      "    accuracy                          0.840      7664\n",
      "   macro avg      0.796     0.693     0.735      7664\n",
      "weighted avg      0.830     0.840     0.830      7664\n",
      "\n",
      "\n",
      "Initializing model for Overall ... inside scaled dot prod attention\n",
      "inside scaled dot prod attention\n",
      "Done\n",
      "\n",
      "Evaluating on test...\n",
      "running fold 2\n",
      "  EPOCH     Tr_LOSS   Tr_F1    Val_LOSS  Val_F1  Tr_bin_loss  Tr_bin_F1  Val_bin_loss  Val_bin_F1\n",
      "--------------------------------------------------------------------------------------\n",
      "      1     349.035   0.193     297.128   0.198    312.408   0.477     272.442   0.466\n",
      "      2     320.054   0.227     291.116   0.247    291.463   0.470     269.092   0.466\n",
      "      3     301.117   0.336     279.544   0.334    276.866   0.470     260.004   0.466\n",
      "      4     289.918   0.418     275.129   0.366    269.914   0.470     260.201   0.466\n",
      "      5     282.071   0.500     271.593   0.398    266.064   0.470     256.108   0.466\n",
      "      6     275.243   0.560     271.306   0.446    261.473   0.470     257.547   0.466\n",
      "      7     280.708   0.574     271.275   0.448    267.943   0.470     259.206   0.466\n",
      "      8     267.862   0.628     268.155   0.461    257.230   0.470     254.863   0.466\n",
      "      9     264.074   0.652     266.810   0.539    254.578   0.470     257.013   0.466\n",
      "     10     258.461   0.683     263.005   0.518    250.776   0.470     253.039   0.466\n",
      "     11     256.275   0.712     266.891   0.493    250.315   0.470     256.949   0.466\n",
      "     12     255.724   0.724     265.870   0.516    249.917   0.477     255.914   0.466\n",
      "     13     253.516   0.735     260.866   0.549    248.997   0.470     252.012   0.466\n",
      "     14     245.813   0.760     264.742   0.547    242.124   0.470     255.772   0.478\n",
      "     15     246.828   0.784     262.816   0.592    244.662   0.471     255.093   0.472\n",
      "     16     240.000   0.784     256.353   0.607    238.130   0.475     248.337   0.470\n",
      "     17     238.922   0.798     263.200   0.585    236.920   0.479     254.792   0.491\n",
      "     18     236.853   0.791     257.948   0.560    235.060   0.486     249.031   0.466\n",
      "     19     236.767   0.801     257.935   0.580    235.754   0.479     249.979   0.481\n",
      "     20     232.188   0.811     266.459   0.607    231.468   0.502     259.462   0.466\n",
      "     21     228.680   0.817     256.186   0.589    227.996   0.495     249.098   0.476\n",
      "     22     228.227   0.819     258.063   0.615    227.694   0.520     250.948   0.477\n",
      "     23     226.641   0.803     259.383   0.590    224.379   0.514     252.349   0.490\n",
      "     24     231.278   0.804     281.856   0.586    230.480   0.548     272.875   0.466\n",
      "     25     231.374   0.805     258.063   0.582    230.210   0.524     250.738   0.517\n",
      "     26     222.270   0.822     264.443   0.552    221.725   0.549     257.708   0.466\n",
      "     27     225.548   0.818     266.999   0.604    225.339   0.526     260.369   0.505\n",
      "     28     220.816   0.836     259.911   0.585    221.175   0.545     253.067   0.507\n",
      "     29     216.350   0.844     265.982   0.598    217.161   0.558     258.471   0.490\n",
      "     30     213.750   0.849     261.953   0.627    214.823   0.575     256.450   0.495\n",
      "     31     208.453   0.868     257.762   0.615    210.031   0.598     250.538   0.493\n",
      "     32     202.817   0.864     263.193   0.625    204.436   0.607     258.418   0.509\n",
      "     33     206.186   0.875     266.846   0.608    208.011   0.601     260.812   0.504\n",
      "     34     206.326   0.870     263.977   0.625    208.173   0.599     258.135   0.508\n",
      "     35     197.878   0.879     276.715   0.617    199.770   0.625     271.029   0.514\n",
      "     36     195.881   0.877     272.117   0.607    197.631   0.642     265.382   0.487\n",
      "     37     189.748   0.882     270.218   0.624    191.466   0.645     264.375   0.523\n",
      "     38     191.589   0.888     279.790   0.610    193.490   0.653     273.178   0.539\n",
      "     39     185.306   0.887     278.580   0.636    187.178   0.657     272.051   0.529\n",
      "     40     183.116   0.894     284.064   0.618    185.261   0.684     279.286   0.507\n",
      "     41     179.608   0.901     275.924   0.608    181.665   0.685     269.351   0.515\n",
      "     42     176.359   0.902     290.786   0.595    178.593   0.697     284.603   0.568\n",
      "     43     171.775   0.903     287.556   0.609    174.118   0.709     281.954   0.525\n",
      "     44     166.165   0.908     287.988   0.600    168.355   0.722     281.783   0.545\n",
      "     45     167.692   0.907     293.092   0.620    170.013   0.716     285.799   0.584\n",
      "     46     173.870   0.897     280.956   0.623    175.935   0.711     274.091   0.552\n",
      "     47     169.391   0.901     305.557   0.606    171.388   0.724     297.802   0.567\n",
      "     48     164.218   0.911     287.252   0.614    166.425   0.737     279.045   0.536\n",
      "     49     157.606   0.888     298.897   0.623    157.860   0.750     290.630   0.561\n",
      "     50     154.211   0.887     303.032   0.595    154.406   0.761     295.852   0.562\n",
      "     51     150.383   0.895     295.946   0.614    151.000   0.770     290.087   0.572\n",
      "     52     145.910   0.903     312.820   0.602    146.980   0.782     308.069   0.560\n",
      "     53     149.344   0.916     305.440   0.631    151.008   0.775     300.356   0.548\n",
      "     54     149.172   0.924     307.877   0.619    151.366   0.776     302.823   0.556\n",
      "     55     138.367   0.923     309.614   0.617    140.099   0.791     304.252   0.583\n",
      "     56     135.966   0.922     325.743   0.606    137.587   0.801     319.882   0.568\n",
      "     57     134.019   0.928     319.503   0.652    135.763   0.803     314.913   0.577\n",
      "     58     134.131   0.924     334.305   0.633    135.903   0.808     329.752   0.566\n",
      "     59     129.621   0.928     333.886   0.620    131.316   0.811     327.521   0.569\n",
      "     60     124.287   0.933     342.296   0.594    125.877   0.826     336.633   0.571\n",
      "     61     125.892   0.935     325.196   0.639    127.760   0.817     319.789   0.572\n",
      "     62     125.550   0.939     334.202   0.624    127.691   0.822     329.023   0.582\n",
      "     63     122.019   0.944     333.082   0.617    124.213   0.828     327.437   0.556\n",
      "     64     122.520   0.947     340.973   0.616    124.830   0.827     334.687   0.574\n",
      "     65     117.828   0.947     341.862   0.638    119.991   0.837     336.746   0.566\n",
      "     66     114.541   0.951     344.482   0.613    116.651   0.840     338.430   0.574\n",
      "     67     114.746   0.950     354.496   0.647    116.951   0.838     349.725   0.579\n",
      "     68     112.598   0.950     353.868   0.622    114.580   0.842     348.128   0.558\n",
      "     69     100.356   0.951     367.731   0.607    101.804   0.868     361.289   0.579\n",
      "     70      97.217   0.952     369.064   0.629     98.605   0.872     362.940   0.578\n",
      "     71      96.104   0.954     365.086   0.617     97.648   0.870     358.322   0.562\n",
      "     72      94.657   0.955     377.952   0.616     96.328   0.870     372.450   0.576\n",
      "     73     104.001   0.956     382.431   0.623    106.028   0.861     377.332   0.567\n",
      "     74     100.789   0.957     374.770   0.623    102.763   0.863     368.238   0.584\n",
      "     75      96.104   0.958     373.809   0.614     97.859   0.871     368.025   0.564\n",
      "Dumping model and data ... Done\n",
      "Time taken: 2308 secs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FAC      0.784     0.777     0.781      1301\n",
      "         RLC      0.619     0.413     0.495        63\n",
      "         RPC      0.903     0.778     0.836       108\n",
      "         ARG      0.652     0.325     0.434       277\n",
      "       Ratio      0.840     0.878     0.859      3589\n",
      "         PRE      0.363     0.418     0.389       366\n",
      "         STA      0.837     0.719     0.774       228\n",
      "\n",
      "    accuracy                          0.789      5932\n",
      "   macro avg      0.714     0.615     0.652      5932\n",
      "weighted avg      0.788     0.789     0.785      5932\n",
      "\n",
      "\n",
      "Initializing model for Overall ... inside scaled dot prod attention\n",
      "inside scaled dot prod attention\n",
      "Done\n",
      "\n",
      "Evaluating on test...\n",
      "running fold 3\n",
      "  EPOCH     Tr_LOSS   Tr_F1    Val_LOSS  Val_F1  Tr_bin_loss  Tr_bin_F1  Val_bin_loss  Val_bin_F1\n",
      "--------------------------------------------------------------------------------------\n",
      "      1     349.161   0.193     290.560   0.207    313.917   0.470     265.817   0.467\n",
      "      2     323.724   0.220     285.773   0.231    296.043   0.470     263.244   0.467\n",
      "      3     305.841   0.331     277.387   0.326    282.245   0.470     257.858   0.467\n",
      "      4     298.690   0.407     264.720   0.462    278.953   0.470     249.404   0.467\n",
      "      5     286.462   0.478     264.032   0.452    270.757   0.475     248.223   0.467\n",
      "      6     281.943   0.516     256.390   0.438    267.300   0.470     241.542   0.467\n",
      "      7     274.023   0.546     283.944   0.422    261.023   0.470     269.176   0.467\n",
      "      8     280.466   0.571     274.354   0.495    270.040   0.470     261.449   0.467\n",
      "      9     275.941   0.627     251.904   0.545    267.268   0.470     241.423   0.467\n",
      "     10     269.548   0.677     256.696   0.504    262.123   0.470     245.606   0.467\n",
      "     11     263.730   0.680     246.881   0.561    256.812   0.470     236.987   0.467\n",
      "     12     258.390   0.701     249.171   0.537    251.843   0.477     239.170   0.482\n",
      "     13     256.582   0.695     245.389   0.574    250.459   0.482     236.384   0.467\n",
      "     14     256.769   0.728     248.416   0.596    252.051   0.476     240.471   0.467\n",
      "     15     249.425   0.759     246.667   0.635    245.983   0.477     239.088   0.467\n",
      "     16     249.462   0.763     243.793   0.616    246.590   0.484     235.846   0.495\n",
      "     17     244.697   0.776     246.208   0.625    242.443   0.500     238.989   0.467\n",
      "     18     252.187   0.778     241.239   0.619    250.588   0.488     233.644   0.467\n",
      "     19     241.192   0.795     243.234   0.589    240.087   0.478     234.566   0.491\n",
      "     20     236.047   0.802     242.369   0.655    235.021   0.513     235.743   0.492\n",
      "     21     235.739   0.813     243.180   0.643    235.150   0.514     236.786   0.529\n",
      "     22     230.218   0.821     243.179   0.625    230.118   0.534     235.754   0.496\n",
      "     23     225.609   0.829     241.864   0.653    225.983   0.557     235.644   0.520\n",
      "     24     222.377   0.824     238.804   0.645    222.324   0.560     231.368   0.513\n",
      "     25     224.008   0.822     242.011   0.629    223.775   0.570     234.400   0.526\n",
      "     26     219.028   0.824     244.629   0.645    218.474   0.590     237.133   0.506\n",
      "     27     214.726   0.829     243.278   0.632    213.924   0.589     236.056   0.533\n",
      "     28     212.242   0.837     254.130   0.582    212.302   0.600     245.653   0.506\n",
      "     29     210.777   0.830     255.462   0.640    210.526   0.613     249.620   0.549\n",
      "     30     211.452   0.842     257.726   0.641    211.952   0.606     250.993   0.580\n",
      "     31     202.685   0.855     250.384   0.635    203.545   0.633     243.974   0.538\n",
      "     32     196.528   0.845     249.975   0.640    196.705   0.644     242.567   0.576\n",
      "     33     203.486   0.854     251.148   0.630    204.163   0.639     243.840   0.491\n",
      "     34     207.712   0.861     251.432   0.635    208.934   0.626     244.734   0.534\n",
      "     35     190.378   0.872     256.874   0.672    191.192   0.662     250.967   0.598\n",
      "     36     188.238   0.879     253.701   0.650    189.541   0.677     247.858   0.582\n",
      "     37     180.884   0.886     256.022   0.665    182.025   0.693     249.946   0.565\n",
      "     38     174.701   0.888     266.250   0.652    175.953   0.712     260.943   0.539\n",
      "     39     170.362   0.882     264.041   0.632    171.360   0.723     257.131   0.592\n",
      "     40     173.080   0.893     262.717   0.682    174.429   0.722     257.265   0.594\n",
      "     41     171.371   0.884     272.179   0.658    172.540   0.731     265.386   0.589\n",
      "     42     165.997   0.888     277.040   0.660    166.918   0.733     270.491   0.581\n",
      "     43     157.056   0.897     278.913   0.649    158.088   0.750     272.921   0.580\n",
      "     44     155.979   0.902     280.142   0.675    157.187   0.763     274.871   0.603\n",
      "     45     153.619   0.905     291.158   0.645    154.999   0.771     284.591   0.557\n",
      "     46     152.746   0.897     298.436   0.617    153.650   0.770     291.733   0.572\n",
      "     47     147.427   0.903     282.211   0.654    148.528   0.776     273.835   0.577\n",
      "     48     138.981   0.905     291.834   0.688    139.554   0.797     286.847   0.576\n",
      "     49     133.549   0.910     311.255   0.666    134.311   0.811     305.370   0.598\n",
      "     50     131.033   0.912     297.807   0.667    131.590   0.811     292.689   0.593\n",
      "     51     132.037   0.903     306.196   0.679    132.243   0.814     302.040   0.607\n",
      "     52     125.611   0.909     309.522   0.673    125.770   0.822     306.530   0.608\n",
      "     53     124.079   0.922     312.142   0.681    124.723   0.833     308.416   0.597\n",
      "     54     125.240   0.923     314.036   0.681    126.192   0.827     309.473   0.581\n",
      "     55     113.845   0.923     308.623   0.695    114.197   0.843     304.925   0.580\n",
      "     56     108.424   0.915     321.629   0.683    108.283   0.856     317.988   0.600\n",
      "     57     105.759   0.925     343.257   0.682    106.027   0.862     339.119   0.576\n",
      "     58     104.017   0.930     334.205   0.647    104.359   0.861     326.663   0.607\n",
      "     59      98.517   0.936     336.545   0.677     98.866   0.874     332.846   0.585\n",
      "     60      93.865   0.935     331.536   0.671     94.047   0.872     326.922   0.575\n",
      "     61      92.933   0.941     338.622   0.689     93.355   0.880     335.818   0.594\n",
      "     62      93.990   0.943     346.898   0.681     94.535   0.886     341.867   0.604\n",
      "     63      91.566   0.944     346.659   0.695     92.233   0.881     342.003   0.607\n",
      "     64      87.242   0.945     373.505   0.679     87.707   0.889     370.960   0.590\n",
      "     65      86.244   0.949     348.911   0.673     86.784   0.890     344.077   0.595\n",
      "     66      83.407   0.948     369.872   0.680     83.729   0.896     365.472   0.582\n",
      "     67      75.173   0.940     385.111   0.665     74.725   0.910     380.470   0.598\n",
      "     68      72.250   0.939     394.347   0.648     71.494   0.914     389.216   0.593\n",
      "     69      71.227   0.941     384.459   0.643     70.529   0.916     380.801   0.593\n",
      "     70      70.912   0.950     397.285   0.686     70.742   0.916     395.136   0.591\n",
      "     71      67.842   0.957     393.572   0.672     67.898   0.919     389.844   0.593\n",
      "     72      66.761   0.955     400.065   0.657     66.707   0.919     397.069   0.589\n",
      "     73      64.141   0.947     399.584   0.654     63.600   0.924     397.000   0.598\n",
      "     74      61.321   0.952     412.726   0.650     60.837   0.930     408.284   0.590\n",
      "     75      70.737   0.943     407.882   0.660     70.084   0.913     403.964   0.593\n",
      "Dumping model and data ... Done\n",
      "Time taken: 2321 secs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FAC      0.812     0.868     0.839      1087\n",
      "         RLC      0.827     0.447     0.580       150\n",
      "         RPC      0.925     0.819     0.869       105\n",
      "         ARG      0.690     0.496     0.577       282\n",
      "       Ratio      0.820     0.937     0.875      3363\n",
      "         PRE      0.810     0.321     0.460       570\n",
      "         STA      0.755     0.599     0.668       242\n",
      "\n",
      "    accuracy                          0.813      5799\n",
      "   macro avg      0.806     0.641     0.695      5799\n",
      "weighted avg      0.811     0.813     0.796      5799\n",
      "\n",
      "\n",
      "Initializing model for Overall ... inside scaled dot prod attention\n",
      "inside scaled dot prod attention\n",
      "Done\n",
      "\n",
      "Evaluating on test...\n",
      "running fold 4\n",
      "  EPOCH     Tr_LOSS   Tr_F1    Val_LOSS  Val_F1  Tr_bin_loss  Tr_bin_F1  Val_bin_loss  Val_bin_F1\n",
      "--------------------------------------------------------------------------------------\n",
      "      1     346.815   0.202     285.429   0.200    314.448   0.479     256.853   0.469\n",
      "      2     322.494   0.210     272.020   0.233    295.133   0.469     248.288   0.469\n",
      "      3     302.875   0.301     266.320   0.398    281.037   0.469     246.136   0.469\n",
      "      4     290.024   0.406     256.346   0.507    271.472   0.469     241.411   0.469\n",
      "      5     291.789   0.474     260.278   0.457    276.589   0.469     246.332   0.469\n",
      "      6     283.376   0.469     253.724   0.526    267.982   0.469     239.146   0.469\n",
      "      7     279.197   0.567     265.159   0.596    266.968   0.469     253.295   0.469\n",
      "      8     275.293   0.600     247.611   0.595    264.847   0.469     236.912   0.469\n",
      "      9     270.351   0.622     249.230   0.537    260.775   0.469     234.959   0.469\n",
      "     10     266.104   0.642     241.915   0.621    257.152   0.469     233.644   0.469\n",
      "     11     260.784   0.661     245.794   0.677    253.350   0.469     239.241   0.469\n",
      "     12     257.486   0.706     239.678   0.665    251.191   0.471     232.386   0.469\n",
      "     13     251.917   0.713     249.910   0.626    246.559   0.472     241.477   0.469\n",
      "     14     257.031   0.715     240.168   0.662    251.648   0.473     233.088   0.469\n",
      "     15     257.050   0.721     240.232   0.672    253.409   0.474     233.661   0.469\n",
      "     16     252.496   0.753     237.696   0.672    249.650   0.470     232.007   0.469\n",
      "     17     247.111   0.767     240.361   0.707    244.415   0.473     235.804   0.469\n",
      "     18     243.564   0.779     234.256   0.685    241.214   0.486     228.925   0.473\n",
      "     19     241.912   0.763     240.523   0.681    239.249   0.490     236.067   0.510\n",
      "     20     239.750   0.790     235.467   0.702    238.594   0.497     231.574   0.483\n",
      "     21     238.245   0.798     242.677   0.696    237.081   0.506     238.308   0.469\n",
      "     22     234.905   0.806     234.871   0.705    234.210   0.496     231.255   0.515\n",
      "     23     228.759   0.815     237.561   0.690    228.639   0.544     232.310   0.534\n",
      "     24     229.766   0.821     244.380   0.650    229.826   0.537     239.860   0.470\n",
      "     25     226.528   0.818     233.027   0.728    226.654   0.539     229.456   0.494\n",
      "     26     230.353   0.833     232.905   0.711    231.253   0.544     229.757   0.469\n",
      "     27     234.337   0.813     238.558   0.720    234.947   0.534     234.922   0.513\n",
      "     28     225.058   0.842     240.001   0.717    226.124   0.555     236.554   0.489\n",
      "     29     225.361   0.831     234.439   0.691    225.736   0.558     230.378   0.497\n",
      "     30     219.057   0.832     235.242   0.730    219.502   0.570     231.976   0.517\n",
      "     31     215.506   0.846     232.902   0.725    216.204   0.592     230.628   0.503\n",
      "     32     210.681   0.853     240.214   0.711    211.965   0.598     237.274   0.527\n",
      "     33     210.684   0.861     239.158   0.722    212.381   0.607     236.428   0.531\n",
      "     34     208.394   0.870     248.183   0.719    210.502   0.611     245.131   0.520\n",
      "     35     204.151   0.867     235.350   0.744    205.972   0.625     233.344   0.502\n",
      "     36     198.346   0.866     247.603   0.723    200.028   0.631     245.261   0.535\n",
      "     37     196.701   0.877     237.413   0.719    198.569   0.640     233.315   0.550\n",
      "     38     196.200   0.881     250.022   0.728    198.464   0.656     247.581   0.531\n",
      "     39     197.911   0.886     241.277   0.735    200.435   0.656     239.011   0.526\n",
      "     40     191.098   0.886     242.751   0.743    193.449   0.672     240.380   0.547\n",
      "     41     189.508   0.888     246.650   0.769    191.746   0.670     245.291   0.551\n",
      "     42     187.892   0.901     247.006   0.722    190.617   0.676     243.912   0.546\n",
      "     43     180.967   0.901     255.798   0.735    183.556   0.705     252.235   0.536\n",
      "     44     177.742   0.903     251.808   0.714    180.370   0.698     248.887   0.564\n",
      "     45     175.221   0.901     261.304   0.742    177.617   0.711     259.477   0.569\n",
      "     46     181.311   0.911     245.636   0.727    184.547   0.698     242.890   0.571\n",
      "     47     177.652   0.906     256.712   0.736    180.496   0.708     253.315   0.545\n",
      "     48     166.847   0.897     253.987   0.730    168.766   0.734     250.143   0.565\n",
      "     49     167.930   0.896     260.742   0.716    169.932   0.735     256.885   0.557\n",
      "     50     169.756   0.891     262.839   0.723    171.341   0.737     258.664   0.556\n",
      "     51     173.693   0.771     273.721   0.556    167.403   0.738     265.278   0.548\n",
      "     52     183.667   0.600     267.944   0.579    171.253   0.741     262.447   0.545\n",
      "     53     186.517   0.644     262.729   0.621    177.032   0.721     256.359   0.571\n",
      "     54     169.614   0.706     262.740   0.656    162.312   0.758     259.016   0.564\n",
      "     55     161.715   0.752     260.769   0.701    156.078   0.761     258.889   0.564\n",
      "     56     154.616   0.781     271.842   0.728    150.804   0.778     270.234   0.563\n",
      "     57     155.424   0.778     265.909   0.718    151.324   0.775     264.895   0.567\n",
      "     58     151.086   0.822     272.327   0.712    148.307   0.790     270.842   0.548\n",
      "     59     150.462   0.827     276.647   0.716    148.356   0.780     275.714   0.564\n",
      "     60     149.523   0.843     271.557   0.729    147.953   0.791     271.055   0.561\n",
      "     61     144.361   0.863     269.427   0.744    143.250   0.786     268.904   0.573\n",
      "     62     145.952   0.874     290.240   0.736    145.758   0.792     289.238   0.578\n",
      "     63     138.271   0.879     277.461   0.730    137.524   0.805     276.503   0.568\n",
      "     64     128.276   0.895     284.651   0.734    127.956   0.824     282.652   0.579\n",
      "     65     126.553   0.900     304.789   0.733    126.449   0.832     305.098   0.549\n",
      "     66     130.912   0.910     282.999   0.745    131.591   0.814     282.678   0.574\n",
      "     67     124.457   0.917     285.833   0.753    125.203   0.837     285.627   0.574\n",
      "     68     123.648   0.917     284.706   0.737    124.493   0.829     283.606   0.593\n",
      "     69     120.299   0.917     290.752   0.744    120.764   0.840     290.335   0.579\n",
      "     70     117.910   0.921     291.319   0.742    118.419   0.839     290.434   0.574\n",
      "     71     117.880   0.920     296.116   0.738    118.447   0.843     295.642   0.561\n",
      "     72     117.249   0.919     300.395   0.747    117.756   0.842     299.179   0.581\n",
      "     73     122.487   0.923     304.639   0.740    123.527   0.839     303.996   0.571\n",
      "     74     132.362   0.925     299.958   0.745    134.076   0.821     298.821   0.595\n",
      "     75     136.756   0.920     296.258   0.728    138.356   0.814     294.646   0.570\n",
      "Dumping model and data ... Done\n",
      "Time taken: 2283 secs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         FAC      0.845     0.868     0.856      1276\n",
      "         RLC      0.712     0.475     0.570        99\n",
      "         RPC      0.876     0.876     0.876       105\n",
      "         ARG      0.680     0.598     0.637       341\n",
      "       Ratio      0.857     0.897     0.877      2875\n",
      "         PRE      0.802     0.713     0.755       848\n",
      "         STA      0.813     0.813     0.813       267\n",
      "\n",
      "    accuracy                          0.835      5811\n",
      "   macro avg      0.798     0.749     0.769      5811\n",
      "weighted avg      0.832     0.835     0.832      5811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Train the model on entire dataset and report loss and macro-F1 after each epoch.\n",
    "'''\n",
    "def learn(model, x_rhet, y_rhet, tag2idx_rhet, x_binary, y_binary, tag2idx_binary, args, val_fold = 0, idx_order = []):\n",
    "\n",
    "    assert idx_order != [], \"empty idx order used\"\n",
    "\n",
    "    samples_per_fold = args.dataset_size // args.num_folds\n",
    "\n",
    "\n",
    "    val_idx = list(range(val_fold * samples_per_fold, val_fold * samples_per_fold + samples_per_fold))\n",
    "    train_idx = list(range(val_fold * samples_per_fold)) + list(range(val_fold * samples_per_fold + samples_per_fold, args.dataset_size))\n",
    "        \n",
    "    train_x_rhet = [x_rhet[i] for i in train_idx]\n",
    "    train_y_rhet = [y_rhet[i] for i in train_idx]\n",
    "    val_x_rhet = [x_rhet[i] for i in val_idx]\n",
    "    val_y_rhet = [y_rhet[i] for i in val_idx]\n",
    "\n",
    "    train_x_binary = [x_binary[i] for i in train_idx]\n",
    "    train_y_binary = [y_binary[i] for i in train_idx]\n",
    "    val_x_binary = [x_binary[i] for i in val_idx]\n",
    "    val_y_binary = [y_binary[i] for i in val_idx]\n",
    "\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.reg)\n",
    "    \n",
    "    print(\"{0:>7}  {1:>10}  {2:>6}  {3:>10}  {4:>6}  {5:>10}  {6:>6}  {7:>10}  {8:>6}\".format('EPOCH', 'Tr_LOSS', 'Tr_F1', 'Val_LOSS', 'Val_F1', 'Tr_bin_loss', 'Tr_bin_F1', 'Val_bin_loss' ,'Val_bin_F1'))\n",
    "    print(\"--------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    best_val_f1 = 0\n",
    "    \n",
    "    model_state = {}\n",
    "    data_state = {}\n",
    "    start_time = time.time()\n",
    "    prev_val_f1 = 0\n",
    "    decrement_ctr = 0\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "\n",
    "        train_loss, train_idx, train_gold, train_pred, train_gold_binary, train_pred_binary, train_rhet_loss, train_binary_loss = train_step(model, opt, train_x_rhet, train_y_rhet, train_x_binary, train_y_binary, args.batch_size)\n",
    "        val_loss, val_idx, val_gold, val_pred, val_gold_binary, val_pred_binary, val_rhet_loss, val_binary_loss = val_step(model, val_x_rhet, val_y_rhet, val_x_binary, val_y_binary, args.batch_size)\n",
    "\n",
    "        train_f1 = f1_score(sum(train_gold, []), sum(train_pred, []), average = 'macro')\n",
    "        val_f1 = f1_score(sum(val_gold, []), sum(val_pred, []), average = 'macro')\n",
    "        train_f1_binary = f1_score(sum(train_gold_binary, []), sum(train_pred_binary, []), average = 'macro')\n",
    "        val_f1_binary = f1_score(sum(val_gold_binary, []), sum(val_pred_binary, []), average = 'macro')\n",
    "\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            model_state = {'epoch': epoch, 'arch': model, 'name': model.__class__.__name__, 'state_dict': model.state_dict(), 'best_f1': val_f1, 'optimizer' : opt.state_dict()}\n",
    "            data_state = {'idx': val_idx, 'loss': val_loss, 'gold': val_gold, 'pred': val_pred}\n",
    "            \n",
    "        if epoch % args.print_every == 0:\n",
    "          print(\"{0:7d}  {1:10.3f}  {2:6.3f}  {3:10.3f}  {4:6.3f} {5:10.3f}  {6:6.3f}  {7:10.3f}  {8:6.3f}\".format(epoch, train_loss, train_f1, val_loss, val_f1, train_binary_loss, train_f1_binary, val_binary_loss, val_f1_binary))\n",
    "\n",
    "          if val_f1 < prev_val_f1 and decrement_ctr>3:\n",
    "            break\n",
    "          elif val_f1 < prev_val_f1:\n",
    "            prev_val_f1 = val_f1\n",
    "            decrement_ctr+=1\n",
    "            \n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"Dumping model and data ...\", end = ' ')\n",
    "    \n",
    "    \n",
    "    print(\"Done\")    \n",
    "    print('Time taken:', int(end_time - start_time), 'secs')\n",
    "\n",
    "    ## Getting results on Val data\n",
    "    statistics(data_state, tag2idx)\n",
    "  \n",
    "\n",
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n",
    "\n",
    "\n",
    "print('\\nPreparing data ...', end = ' ')\n",
    "\n",
    "x_rhet, y_rhet, word2idx, tag2idx = prepare_data_new(idx_order, args, data_path = args.data_path_rr)\n",
    "\n",
    "## Emb dim is 3 times because input for shift module -> concat(shift embedding of current and previous sentence, pre-trained emb of curr sentence(For RR module this is the only input), shift emb of current and next sentence)\n",
    "x_binary, y_binary, word2idx_binary, tag2idx_binary = prepare_data_new(idx_order, args, data_path = args.data_path_binary, data_type = 'binary')\n",
    "\n",
    "\n",
    "\n",
    "print('Done')\n",
    "\n",
    "print('#Tags Overall:', len(tag2idx))\n",
    "\n",
    "print('#Tags Overall binary:', len(tag2idx_binary))\n",
    "\n",
    "print('Dump word2idx and tag2idx')\n",
    "with open(args.save_path + 'word2idx.json', 'w') as fp:\n",
    "    json.dump(word2idx, fp)\n",
    "with open(args.save_path + 'tag2idx.json', 'w') as fp:\n",
    "    json.dump(tag2idx, fp)\n",
    "    \n",
    "with open(args.save_path + 'word2idx_binary.json', 'w') as fp:\n",
    "    json.dump(word2idx_binary, fp)\n",
    "with open(args.save_path + 'tag2idx_binary.json', 'w') as fp:\n",
    "    json.dump(tag2idx_binary, fp)\n",
    "\n",
    "\n",
    "\n",
    "for fold in range(fold_num,5):\n",
    "  print('\\nInitializing model for Overall ...', end = ' ')   \n",
    "  model = MTL_Classifier(len(tag2idx), args.emb_dim, tag2idx['<start>'], tag2idx['<end>'], tag2idx['<pad>'], vocab_size = len(word2idx), pretrained = args.pretrained, device = args.device, use_attention = args.use_attention).to(args.device)\n",
    "  print('Done')\n",
    "  print('\\nEvaluating on test...')  \n",
    "  print(\"running fold {}\".format(fold))\n",
    "  learn(model, x_rhet, y_rhet, tag2idx, x_binary, y_binary, tag2idx_binary, args, idx_order = idx_order, val_fold = fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM9YolpjYvNf"
   },
   "outputs": [],
   "source": [
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkpQHKIkOuOI"
   },
   "outputs": [],
   "source": [
    "from google.colab import runtime\n",
    "runtime.unassign()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMpP+/esLaelJLTQNy3i11X",
   "machine_shape": "hm",
   "mount_file_id": "1AAKM0VGchbyQjKwYQnArZ5fDozVFAlah",
   "provenance": [
    {
     "file_id": "1AAKM0VGchbyQjKwYQnArZ5fDozVFAlah",
     "timestamp": 1676727875709
    },
    {
     "file_id": "1HI5HzP80XpMkeQHlMTV_Lb6w1SA4bHCe",
     "timestamp": 1665854616627
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "039b1fad52fb408a90bdc709eabe7642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_277b11c516544b8c9c9cf311acc6aec7",
      "placeholder": "​",
      "style": "IPY_MODEL_91f0b2f5c53143a39bdbc5e6e401a6fd",
      "value": "Downloading: 100%"
     }
    },
    "0e23dbafd79645208a4e3f3d93e5adf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1a5a514166fa4e7a95ba5056575f6650": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a478037f98cb4ff896e916398c34c2c3",
       "IPY_MODEL_af9c483bed814ef2afe2ce2fa22ae2ad",
       "IPY_MODEL_cfb19276ebed4e89bec10bf8baf48d3a"
      ],
      "layout": "IPY_MODEL_28cb0db6f1dc4c1cada2d753113b42e5"
     }
    },
    "1c184ea55e2c4c91b1e339fd03ded3af": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1ed456ab88e74ad3bc4ce9c9901771ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c8286db66aeb41e6904d0361d739bfb4",
       "IPY_MODEL_f14b6d1bf1de457b910b75d3833867b0",
       "IPY_MODEL_863921a12ead4cd5a4c9612f9145d0a2"
      ],
      "layout": "IPY_MODEL_9a2b8b70eaa54ee69d47b062af5cd5a2"
     }
    },
    "277b11c516544b8c9c9cf311acc6aec7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28cb0db6f1dc4c1cada2d753113b42e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2bde3aee2aeb4de482b5621716fc1163": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30425f1dce484a6e80572a7965999103": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f6acdda010b0486ea9497b2795294c3b",
      "placeholder": "​",
      "style": "IPY_MODEL_658cce8da84448bea88b281eb8d4a857",
      "value": "Downloading: 100%"
     }
    },
    "336e2b56c9fe48c6bc53d1f059a8da67": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aabfc5afb31d40709ed4dcf7128a1965",
      "placeholder": "​",
      "style": "IPY_MODEL_788d5c013441489aad124ff9e60909b8",
      "value": " 989/989 [00:00&lt;00:00, 30.8kB/s]"
     }
    },
    "3499bf73f6db458fb62d81abff324420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_91142443e2984c5ba8a302c5a0a56a0b",
      "max": 989,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e5a47991c7640e1873134784203c11d",
      "value": 989
     }
    },
    "430a3c25877640d98a7ea30b724f6cfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "43730558f55447509dbd6bd47bfaaeac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e5a47991c7640e1873134784203c11d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "658cce8da84448bea88b281eb8d4a857": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e2d9d9f5c9346bdb379771451ce8579": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87adea8c5a4f47999722abf1959125ed",
      "placeholder": "​",
      "style": "IPY_MODEL_a96b18980f164452a3b6c24e560ed62a",
      "value": " 48.0/48.0 [00:00&lt;00:00, 1.73kB/s]"
     }
    },
    "7427b98095244a54b223e102bef31b1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75cd49754b924524b54327ffaae744c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "771c5c55fb144e959826234b3c8275f0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "788d5c013441489aad124ff9e60909b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "863921a12ead4cd5a4c9612f9145d0a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_43730558f55447509dbd6bd47bfaaeac",
      "placeholder": "​",
      "style": "IPY_MODEL_0e23dbafd79645208a4e3f3d93e5adf0",
      "value": " 222k/222k [00:00&lt;00:00, 602kB/s]"
     }
    },
    "87adea8c5a4f47999722abf1959125ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91142443e2984c5ba8a302c5a0a56a0b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "91f0b2f5c53143a39bdbc5e6e401a6fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "942357d76bc348cdb1997435ca34bb38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9a2b8b70eaa54ee69d47b062af5cd5a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9e10223f73ad4eac821e64ddfb358c67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a478037f98cb4ff896e916398c34c2c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e10223f73ad4eac821e64ddfb358c67",
      "placeholder": "​",
      "style": "IPY_MODEL_dd82a38d534d41a3825caed3fb602228",
      "value": "Downloading: 100%"
     }
    },
    "a7bdf693bb0c4ed29c631c4ce168411a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_30425f1dce484a6e80572a7965999103",
       "IPY_MODEL_b910d4d50de14566935a0937de30ab66",
       "IPY_MODEL_6e2d9d9f5c9346bdb379771451ce8579"
      ],
      "layout": "IPY_MODEL_de43a10bc6724dd9ada9cf17b2be0d98"
     }
    },
    "a96b18980f164452a3b6c24e560ed62a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aabfc5afb31d40709ed4dcf7128a1965": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af9c483bed814ef2afe2ce2fa22ae2ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7427b98095244a54b223e102bef31b1a",
      "max": 141480422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c384cd731a7d456e945b0e578b4c073f",
      "value": 141480422
     }
    },
    "b910d4d50de14566935a0937de30ab66": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2bde3aee2aeb4de482b5621716fc1163",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d54b6db4c14f47f4b4caeb55e22cf555",
      "value": 48
     }
    },
    "c384cd731a7d456e945b0e578b4c073f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c8286db66aeb41e6904d0361d739bfb4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_430a3c25877640d98a7ea30b724f6cfc",
      "placeholder": "​",
      "style": "IPY_MODEL_f50ba7b51e1c4a1a93ece757c460a956",
      "value": "Downloading: 100%"
     }
    },
    "ce08c87dc91a42c98ca1bd9d219153d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_039b1fad52fb408a90bdc709eabe7642",
       "IPY_MODEL_3499bf73f6db458fb62d81abff324420",
       "IPY_MODEL_336e2b56c9fe48c6bc53d1f059a8da67"
      ],
      "layout": "IPY_MODEL_75cd49754b924524b54327ffaae744c3"
     }
    },
    "cfb19276ebed4e89bec10bf8baf48d3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_771c5c55fb144e959826234b3c8275f0",
      "placeholder": "​",
      "style": "IPY_MODEL_f79e154945ae488493cb978128576567",
      "value": " 141M/141M [00:02&lt;00:00, 60.6MB/s]"
     }
    },
    "d54b6db4c14f47f4b4caeb55e22cf555": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dd82a38d534d41a3825caed3fb602228": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de43a10bc6724dd9ada9cf17b2be0d98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f14b6d1bf1de457b910b75d3833867b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c184ea55e2c4c91b1e339fd03ded3af",
      "max": 221792,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_942357d76bc348cdb1997435ca34bb38",
      "value": 221792
     }
    },
    "f50ba7b51e1c4a1a93ece757c460a956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f6acdda010b0486ea9497b2795294c3b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f79e154945ae488493cb978128576567": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
